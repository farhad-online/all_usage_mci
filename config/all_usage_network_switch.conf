all_usage {
  network_switch {
    name = "network_switch"
    env = "production"
    enabled = true

    spark {
      appName = "ods_all_usage_network_switch"
      master = "yarn"
      checkpointLocation = "/user/spark/checkpoints/ods_all_usage_network_switch"
      format = "hive"
      outputMode = "append"
      batchMode = "append"
      batchFormat = "parquet"
      triggerInterval = 5
      options = {
        "spark.sql.adaptive.enabled" = "true"
        "spark.sql.adaptive.coalescePartitions.enabled" = "true"
      }
    }

    sparkKafkaConsumer {
      foramt = "kafka"
      options = {
        "subscribe" = "network"
        "startingOffsets" = "latest"
        "kafka.bootstap.servers" = "master.dwbi.mci:9092"
        "kafka.security.protocol" = "SASL_PLAINTEXT"
        "kafka.sasl.kerberos.service.name" = "kafka"
        "kafka.spark.streaming.kafka.consumer.cache.enabled" = "false"
      }
    }

    sparkHive {
      tableName = "ods.all_usage"
      partitionKey = "day_key"
      options = {
        "hive.exec.compress.output" = "true"
      }
    }
  }
}
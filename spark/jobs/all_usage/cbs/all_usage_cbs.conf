all_usage {
   cbs {
    name = "cbs"
    env = "production"
    enabled = true

    spark {
      appName = "ods_all_usage_cbs"
      master = "yarn"
      checkpointLocation = "/user/spark/checkpoints/ods_all_usage_cbs"
      format = "hive"
      outputMode = "append"
      batchMode = "append"
      batchFormat = "parquet"
      triggerInterval = 1
      options = {
        spark.driver.extraJavaOptions = "-Djava.security.auth.login.config=spark_jaas.conf"
        spark.executor.extraJavaOptions = "-Djava.security.auth.login.config=spark_jaas.conf"
        spark.sql.adaptive.enabled = "true"
        spark.sql.adaptive.coalescePartitions.enabled = "true"
        spark.executor.memory = "1g"
        spark.executor.cores = "1"
        spark.driver.memory = "2g"
        spark.yarn.am.memory = "1g"
        spark.yarn.driver.memoryOverhead = "1g"
        spark.executor.instances = "2"
        spark.network.timeout = "1200s"
        hive.exec.dynamic.partition = "true"
        mapred.output.compress = "true"
        mapred.compress.map.output = "true"
        hive.exec.dynamic.partition.mode = "nonstrict"
        spark.sql.broadcastTimeout = "6000"
        spark.sql.hive.convertMetastoreParquet = "false"
        spark.sql.hive.convertMetastoreOrc = "false"
        spark.streaming.backpressure.enabled = "true"
        spark.streaming.backpressure.initialRate = "100"
        spark.streaming.kafka.maxRatePerPartition = "100"
        spark.hadoop.hive.metastore.sasl.enabled = "true"
      }
    }

    sparkKafkaConsumer {
      format = "kafka"
      options = {
        subscribe = "all_cbs_ocs,all_cbs_postpaid"
        startingOffsets = "earliest"
        kafka.bootstrap.servers = "master.dwbi.mci:9092"
        kafka.security.protocol = "SASL_PLAINTEXT"
        kafka.sasl.kerberos.service.name = "kafka"
        kafka.spark.streaming.kafka.consumer.cache.enabled = "false"
        maxOffsetPerTrigger = "100"
      }
    }

    sparkHive {
      tableName = "default.all_usage"
      partitionKey = "start_date"
      options = {
        hive.exec.compress.output = "true"
        hive.exec.copyfile.maxsize = "100000000000000"
        hive.exec.max.created.files = "10000000"
      }
    }
  }
}

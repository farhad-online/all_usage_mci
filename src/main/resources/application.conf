app {
  appName = "MySparkApp"
  environment = "development"
  version = "1.0.0"

  spark {
    appName = "MySparkApp"
    master = "local[*]"
    executorMemory = "2g"
    driverMemory = "1g"
    executorCores = 2
    maxResultSize = "1g"
    serializer = "org.apache.spark.serializer.KryoSerializer"
    dynamicAllocation = false
    checkpointLocation = "/tmp/spark-checkpoints"
    additionalConfigs = {
      "spark.sql.adaptive.enabled" = "true"
      "spark.sql.adaptive.coalescePartitions.enabled" = "true"
    }
  }

  kafka {
    bootstrapServers = "localhost:9092"
    groupId = "spark-consumer-group"
    autoOffsetReset = "latest"
    enableAutoCommit = false
    maxPollRecords = 500
    sessionTimeoutMs = 30000
    topics = ["input-topic", "events-topic"]
    customProperties = {
      "security.protocol" = "SASL_SSL"
      "sasl.mechanism" = "PLAIN"
    }
  }

  kafkaProducer {
    bootstrapServers = "localhost:9092"
    acks = "all"
    retries = 3
    batchSize = 16384
    lingerMs = 1
    bufferMemory = 33554432
  }

  hive {
    metastoreUri = "thrift://localhost:9083"
    warehouse = "/user/hive/warehouse"
    database = "analytics"
    enableStats = true
    enableCBO = true
    dynamicPartitioning = true
    maxDynamicPartitions = 1000
    compressionCodec = "snappy"
  }

  subApps {
    "data-processor" {
      name = "data-processor"
      enabled = true
      kafka {
        bootstrapServers = "localhost:9092"
        groupId = "data-processor-group"
        topics = ["raw-data"]
      }
      hive {
        database = "processed_data"
        metastoreUri = "thrift://localhost:9083"
        warehouse = "/user/hive/warehouse"
      }
    }

    "ml-pipeline" {
      name = "ml-pipeline"
      enabled = true
      spark {
        appName = "ML-Pipeline"
        executorMemory = "4g"
        driverMemory = "2g"
        additionalConfigs = {
          "spark.ml.enabled" = "true"
        }
      }
    }
  }
}